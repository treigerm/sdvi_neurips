{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn.apionly as sns\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import hydra\n",
    "import omegaconf\n",
    "import csv\n",
    "import torch\n",
    "\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.gp as gp\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from models.pyro_extensions.infer import SDVI\n",
    "from models.pyro_extensions.resource_allocation import SuccessiveHalving\n",
    "from models import normal_model\n",
    "from run_baselines import NormalModel\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "matplotlib.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycler import cycler\n",
    "line_cycler = (cycler(color=[\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#F0E442\"]) +\n",
    "               cycler(linestyle=[\"-\", \"--\", \"-.\", \":\", \"-\", \"--\", \"-.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo2config = {\n",
    "    \"DCC\": {\"color\": \"#E69F00\", \"linestyle\": \"--\"},\n",
    "    \"BBVI\": {\"color\": \"#56B4E9\", \"linestyle\": \"-.\"},\n",
    "    \"Pyro AutoGuide\": {\"color\": \"#009E73\", \"linestyle\": \":\"},\n",
    "    \"SDVI\": {\"color\": \"#D55E00\", \"linestyle\": \"-\"},\n",
    "    \"Stochastic SDVI\": {\"color\": \"#0072B2\", \"linestyle\": \":\"},\n",
    "    \"S-SDVI\": {\"color\": \"#0072B2\", \"linestyle\": \":\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major = 8.0\n",
    "minor = 5.0\n",
    "\n",
    "major_tick_width = 2.0\n",
    "\n",
    "update_rc_params = {\n",
    "    'font.family': \"serif\", \n",
    "    'font.size': 24, \n",
    "    'legend.fontsize': 15,\n",
    "    'text.usetex': True,\n",
    "    'xtick.major.size': major,\n",
    "    'xtick.major.width': major_tick_width,\n",
    "    'xtick.minor.size': minor,\n",
    "    'ytick.major.size': major,\n",
    "    'ytick.major.width': major_tick_width,\n",
    "    'ytick.minor.size': minor,\n",
    "    'axes.linewidth': 2.0,\n",
    "}\n",
    "\n",
    "plt.rcParams.update(update_rc_params)\n",
    "plt.rc(\"axes\", prop_cycle=line_cycler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH1_PRIOR_MEAN = -3\n",
    "BRANCH2_PRIOR_MEAN = 3\n",
    "PRIOR_STD = 1\n",
    "LIKELIHOOD_STD = 2\n",
    "\n",
    "OBSERVED_DATA = 2\n",
    "\n",
    "def marginal_likelihood(data, likelihood_std, prior_mean, prior_std):\n",
    "    \"\"\"Calculate the marginal likelihood of a branch. Assumes we observe only\n",
    "    a single data point.\n",
    "\n",
    "    Taken from Section 2.5 at https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf.\n",
    "    \"\"\"\n",
    "    likelihood_var = math.pow(likelihood_std, 2)\n",
    "    prior_var = math.pow(prior_std, 2)\n",
    "\n",
    "    first_term = likelihood_std / (\n",
    "        (math.sqrt(2 * math.pi) * likelihood_std)\n",
    "        * math.sqrt(prior_var + likelihood_var)\n",
    "    )\n",
    "    second_term = math.exp(\n",
    "        -(math.pow(data, 2) / (2 * likelihood_var))\n",
    "        - (math.pow(prior_mean, 2) / (2 * prior_var))\n",
    "    )\n",
    "    third_term = math.exp(\n",
    "        (\n",
    "            (prior_var * math.pow(data, 2) / likelihood_var)\n",
    "            + (likelihood_var * math.pow(prior_mean, 2) / prior_var)\n",
    "            + 2 * data * prior_mean\n",
    "        )\n",
    "        / (2 * (prior_var + likelihood_var))\n",
    "    )\n",
    "    return first_term * second_term * third_term\n",
    "\n",
    "\n",
    "def posterior_params(data, likelihood_std, prior_mean, prior_std):\n",
    "    \"\"\"Calculate the posterior mean and standard deviation of a branch. Assumes we\n",
    "    observe only a single data point.\"\"\"\n",
    "    prior_precision = 1 / math.pow(prior_std, 2)\n",
    "    likelihood_precision = 1 / math.pow(likelihood_std, 2)\n",
    "    post_mean = (prior_precision * prior_mean + likelihood_precision * data) / (\n",
    "        prior_precision + likelihood_precision\n",
    "    )\n",
    "    post_std = 1 / (prior_precision + likelihood_precision)\n",
    "    return post_mean, post_std\n",
    "\n",
    "class ToyModel:\n",
    "    observed_data = torch.tensor(OBSERVED_DATA)\n",
    "\n",
    "    branch1_prior_mean = BRANCH1_PRIOR_MEAN\n",
    "    branch2_prior_mean = BRANCH2_PRIOR_MEAN\n",
    "    prior_std = PRIOR_STD\n",
    "    likelihood_std = LIKELIHOOD_STD\n",
    "\n",
    "    def __init__(self, cut_point=0.0):\n",
    "        self.branch1_post_mean, self.branch1_post_std = posterior_params(\n",
    "            self.observed_data,\n",
    "            self.likelihood_std,\n",
    "            self.branch1_prior_mean,\n",
    "            self.prior_std,\n",
    "        )\n",
    "        self.branch1_Z = marginal_likelihood(\n",
    "            self.observed_data,\n",
    "            self.likelihood_std,\n",
    "            self.branch1_prior_mean,\n",
    "            self.prior_std,\n",
    "        )\n",
    "        self.branch2_post_mean, self.branch2_post_std = posterior_params(\n",
    "            self.observed_data,\n",
    "            self.likelihood_std,\n",
    "            self.branch2_prior_mean,\n",
    "            self.prior_std,\n",
    "        )\n",
    "        self.branch2_Z = marginal_likelihood(\n",
    "            self.observed_data,\n",
    "            self.likelihood_std,\n",
    "            self.branch2_prior_mean,\n",
    "            self.prior_std,\n",
    "        )\n",
    "\n",
    "        self.cut_point = cut_point\n",
    "\n",
    "        z0_prior = dist.Normal(0, 1)\n",
    "        self.branch1_prior = z0_prior.cdf(torch.tensor(cut_point))\n",
    "        self.marginal_likelihood = self.branch1_prior * self.branch1_Z + (1 - self.branch1_prior) * self.branch2_Z\n",
    "\n",
    "        self.branch1_post_prob = (self.branch1_prior * self.branch1_Z) / self.marginal_likelihood\n",
    "\n",
    "    def __call__(self):\n",
    "        z0 = pyro.sample(\"z0\", dist.Normal(0, 1))\n",
    "        if z0 < self.cut_point:\n",
    "            z1 = pyro.sample(\"z1\", dist.Normal(self.branch1_prior_mean, self.prior_std))\n",
    "        else:\n",
    "            z1 = pyro.sample(\"z2\", dist.Normal(self.branch2_prior_mean, self.prior_std))\n",
    "\n",
    "        x = pyro.sample(\n",
    "            \"x\", dist.Normal(z1, self.likelihood_std), obs=self.observed_data\n",
    "        )\n",
    "        return z0.item(), z1, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_weights(file_list):\n",
    "    weight_keys = [\"br2_weights_pyro\", \"br2_weights_bbvi\", \"br2_weights_sdvi\"]\n",
    "    weights_dict = {\n",
    "        \"br2_weights_pyro\": [],\n",
    "        \"br2_weights_bbvi\": [],\n",
    "        \"br2_weights_sdvi\": [],\n",
    "    }\n",
    "\n",
    "    for filename in file_list:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            weights = pickle.load(f)\n",
    "        for k in weight_keys:\n",
    "            weights_dict[k].append(weights[k])\n",
    "    \n",
    "    for k in weight_keys:\n",
    "        weights_dict[k] = torch.cat(weights_dict[k], 0)\n",
    "    \n",
    "    return weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file_list = [\n",
    "    # TODO: Fill out with list of files which are output of ../scripts/make_motivating_example_plot.py.\n",
    "]\n",
    "\n",
    "weights_dict = merge_weights(weights_file_list)\n",
    "\n",
    "br2_weights_pyro = weights_dict[\"br2_weights_pyro\"]\n",
    "br2_weights_bbvi = weights_dict[\"br2_weights_bbvi\"]\n",
    "br2_weights_sdvi = weights_dict[\"br2_weights_sdvi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo2config = {\n",
    "    \"BBVI\": {\"color\": \"#56B4E9\", \"linestyle\": \"-.\"},\n",
    "    \"Pyro\": {\"color\": \"#009E73\", \"linestyle\": \":\"},\n",
    "    \"SDVI\": {\"color\": \"#D55E00\", \"linestyle\": \"-\"},\n",
    "}\n",
    "\n",
    "algo2config = {\n",
    "    \"BBVI\": {\"color\": \"#56B4E9\"},\n",
    "    \"Pyro\": {\"color\": \"#009E73\"},\n",
    "    \"SDVI\": {\"color\": \"#D55E00\"},\n",
    "}\n",
    "\n",
    "font = {'size' : 16}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "update_rc_params = {\n",
    "    'font.family': \"serif\", \n",
    "    'text.usetex': True,\n",
    "    'font.size': 20, \n",
    "    'legend.fontsize': 15,\n",
    "}\n",
    "plt.rcParams.update(update_rc_params)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "#  Pyro weights\n",
    "weight_mean = br2_weights_pyro.mean(dim=0).numpy()\n",
    "weight_std = br2_weights_pyro.std(dim=0).numpy()\n",
    "ax.plot(weight_mean, lw=4, label=\"Pyro AutoGuide\", **algo2config[\"Pyro\"])\n",
    "ax.fill_between(\n",
    "    torch.arange(weight_mean.shape[0]),\n",
    "    weight_mean - weight_std,\n",
    "    weight_mean + weight_std,\n",
    "    alpha=0.5,\n",
    "    color=algo2config[\"Pyro\"][\"color\"],\n",
    ")\n",
    "\n",
    "#  BBVI weights\n",
    "weight_mean = br2_weights_bbvi.mean(dim=0).numpy()\n",
    "weight_std = br2_weights_bbvi.std(dim=0).numpy()\n",
    "ax.plot(weight_mean, lw=4, label=\"BBVI\", **algo2config[\"BBVI\"])\n",
    "ax.fill_between(\n",
    "    torch.arange(weight_mean.shape[0]),\n",
    "    weight_mean - weight_std,\n",
    "    weight_mean + weight_std,\n",
    "    alpha=0.5,\n",
    "    color=algo2config[\"BBVI\"][\"color\"],\n",
    ")\n",
    "\n",
    "#  SDVI weights\n",
    "weight_mean = br2_weights_sdvi[:,::8].mean(dim=0).numpy()\n",
    "weight_std = br2_weights_sdvi[:,::8].std(dim=0).numpy()\n",
    "ax.plot(torch.arange(weight_mean.shape[0]) * 16, weight_mean, lw=4, label=\"SDVI\", **algo2config[\"SDVI\"])\n",
    "ax.fill_between(\n",
    "    torch.arange(weight_mean.shape[0]) * 16,\n",
    "    weight_mean - weight_std,\n",
    "    weight_mean + weight_std,\n",
    "    alpha=0.5,\n",
    "    color=algo2config[\"SDVI\"][\"color\"],\n",
    ")\n",
    "\n",
    "model = ToyModel(cut_point=0.0)\n",
    "branch2_prob = (1 - model.branch1_post_prob)\n",
    "ax.axhline(branch2_prob, ls=\"--\", color=\"black\", lw=4, label=\"Ground Truth\")\n",
    "\n",
    "ax.set_xlabel(\"Number of Iterations\")\n",
    "ax.set_ylabel(r\"Probability of Branch $x \\geq 0$\")\n",
    "ax.set_yticks([0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "# ax.set_xlim(0, 1000)\n",
    "x0, y0, width, height = 0.3, 0.3, 0.4, 0.1\n",
    "ax.legend(loc=\"center\", bbox_to_anchor=(x0, y0, width, height), ncol=2)\n",
    "# ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(\n",
    "    os.path.join(\"figures\", \"motivating_example_weights_sdvi.pdf\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection_gtws, _ = NormalModel().calculate_ground_truth_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_extract_errors(sweep_dir, ground_truth_weights, method_name, fname=\"estimated_weights.csv\"):\n",
    "    slp_identifiers = list(ground_truth_weights.keys())\n",
    "    ground_truth_array = np.stack(\n",
    "        [np.array(ground_truth_weights[a]) for a in slp_identifiers], axis=0\n",
    "    )\n",
    "\n",
    "    experiment_dirs = [f.path for f in os.scandir(sweep_dir) if f.is_dir()]\n",
    "    dfs = []\n",
    "    for experiment_dir in experiment_dirs:\n",
    "        run_id = os.path.basename(experiment_dir)\n",
    "        if run_id == \".submitit\":\n",
    "            continue\n",
    "        \n",
    "        data = np.genfromtxt(os.path.join(experiment_dir, fname), delimiter=\",\", names=True)\n",
    "        # Make sure columns are ordered as in ground_truth_array\n",
    "        data = data[slp_identifiers]\n",
    "        # For each iteration calcluate error\n",
    "        num_iterations = data.shape[0]\n",
    "        errors = np.zeros(num_iterations)\n",
    "        for ix in range(num_iterations):\n",
    "            row_vals = np.zeros(len(slp_identifiers))\n",
    "            for j, id in enumerate(slp_identifiers):\n",
    "                row_vals[j] = data[id][ix]\n",
    "            errors[ix] = np.linalg.norm(ground_truth_array - row_vals) ** 2\n",
    "\n",
    "        # Create pandas dataframe with iteration,error as columns and append to dfs\n",
    "        errors_pd = pd.DataFrame(data={\"weight_error\": errors, \"iteration\": np.arange(num_iterations)})\n",
    "        errors_pd[\"run_id\"] = int(run_id)\n",
    "        dfs.append(errors_pd)\n",
    "    \n",
    "    # Concat all dataframes to each other\n",
    "    metrics = pd.concat(dfs)\n",
    "\n",
    "    # Group-by iteration id and calculate mean and standard deviation so that we get a table \"iteration,mean,std\"\n",
    "    grouped_by_iteration = metrics[[\"iteration\", \"weight_error\"]].groupby(\"iteration\")\n",
    "    losses_stats = grouped_by_iteration.mean()\n",
    "    losses_stats.rename(columns={\"weight_error\": \"mean\"}, inplace=True)\n",
    "    losses_stats[\"std\"] = grouped_by_iteration.std()[\"weight_error\"]\n",
    "    losses_stats[\"method_name\"] = method_name\n",
    "    return losses_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdvi_extract_errors(sweep_dir, ground_truth_weights, method_name, fname=\"exclusive_kl_results.csv\"):\n",
    "    slp_identifiers = list(ground_truth_weights.keys())\n",
    "    ground_truth_array = np.stack(\n",
    "        [np.array(ground_truth_weights[a]) for a in slp_identifiers], axis=0\n",
    "    )\n",
    "\n",
    "    # Loop through the different run idxs\n",
    "    experiment_dirs = [f.path for f in os.scandir(sweep_dir) if f.is_dir()]\n",
    "    dfs = []\n",
    "    for experiment_dir in experiment_dirs:\n",
    "        run_id = os.path.basename(experiment_dir)\n",
    "        if run_id == \".submitit\":\n",
    "            continue\n",
    "        \n",
    "        # Load metrics csv into pandas dataframe\n",
    "        metrics = pd.read_csv(os.path.join(experiment_dir, fname))\n",
    "\n",
    "        num_iterations = len(metrics.index)\n",
    "        errors = np.zeros(num_iterations)\n",
    "        for ix in range(num_iterations):\n",
    "            row_vals = np.zeros(len(slp_identifiers))\n",
    "            for j, id in enumerate(slp_identifiers):\n",
    "                # bt = int(id) * \"0\" + \"1\"\n",
    "                bt = f\"u,x_{int(id)}\"\n",
    "                row_vals[j] = metrics[f\"weight_{bt}\"][ix]\n",
    "            errors[ix] = np.linalg.norm(ground_truth_array - row_vals) ** 2\n",
    "\n",
    "        # Create pandas dataframe with iteration,error as columns and append to dfs\n",
    "        errors_pd = pd.DataFrame(data={\"weight_error\": errors, \"iteration\": np.arange(num_iterations)})\n",
    "        errors_pd[\"run_id\"] = int(run_id)\n",
    "        dfs.append(errors_pd)\n",
    "\n",
    "    # Concat all dataframes to each other\n",
    "    metrics = pd.concat(dfs)\n",
    "    metrics.describe()\n",
    "\n",
    "    # Group-by iteration id and calculate mean and standard deviation so that we get a table \"iteration,mean,std\"\n",
    "    grouped_by_iteration = metrics[[\"iteration\", \"weight_error\"]].groupby(\"iteration\")\n",
    "    losses_stats = grouped_by_iteration.mean()\n",
    "    losses_stats.rename(columns={\"weight_error\": \"mean\"}, inplace=True)\n",
    "    losses_stats[\"std\"] = grouped_by_iteration.std()[\"weight_error\"]\n",
    "    losses_stats[\"method_name\"] = method_name\n",
    "    return losses_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdvi_dirs = [\n",
    "    (\"SDVI\", \"TODO: Fill out path to results dir.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = [\n",
    "    (\"DCC\", \"TODO: Fill out path to results dir.\"),\n",
    "    (\"Pyro AutoGuide\", \"TODO: Fill out path to results dir.\"),\n",
    "    (\"BBVI\", \"TODO: Fill out path to results dir.\"),\n",
    "]\n",
    "\n",
    "method2errors = {\n",
    "    n: baseline_extract_errors(sweep_dir, model_selection_gtws, n)\n",
    "    for n, sweep_dir in baselines\n",
    "}\n",
    "for n, sweep_dir in sdvi_dirs:\n",
    "    method2errors[n] = sdvi_extract_errors(sweep_dir, model_selection_gtws, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(8,4))\n",
    "fig, ax = plt.subplots()\n",
    "for name, errors in method2errors.items():\n",
    "    ixs = errors.index\n",
    "    if name == \"DCC\":\n",
    "        ixs = ixs * 1000 + 10000\n",
    "        start_ix = 0\n",
    "    elif name == \"SDVI\":\n",
    "        ixs = ixs * 500 + 2000\n",
    "        start_ix = 0\n",
    "    elif name == \"Pyro AutoGuide\":\n",
    "        ixs = ixs * 1000\n",
    "        start_ix = 0\n",
    "    elif name == \"BBVI\":\n",
    "        ixs = ixs * 1000\n",
    "        start_ix = 0\n",
    "    ax.plot(\n",
    "        ixs[start_ix:], \n",
    "        errors[\"mean\"][start_ix:], \n",
    "        label=name, \n",
    "        alpha=1.0, \n",
    "        lw=4, \n",
    "        **algo2config[name]\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        ixs[start_ix:], \n",
    "        errors[\"mean\"][start_ix:]-errors[\"std\"][start_ix:], \n",
    "        errors[\"mean\"][start_ix:]+errors[\"std\"][start_ix:], \n",
    "        alpha=0.3,\n",
    "        **algo2config[name]\n",
    "    )\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Computational Cost\")\n",
    "ax.set_ylabel(\"Squared Error\")\n",
    "# ax.set_ylim((0.0, 0.05))\n",
    "# ax.set_xlim((10^4, 10^5))\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlim((1000, 100000))\n",
    "# ax.set_ylim(ymin=1e-5)\n",
    "ax.grid(True)\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"figures/model_selection_slp_weight_error_without_dcc_different_aspect_ratio.pdf\")\n",
    "fig.savefig(\"figures/model_selection_slp_weight_error.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELBO with Marginal Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = normal_model.NormalModel()\n",
    "sdvi = SDVI(model, 0.1, \"MeanFieldNormal\", utility_class=SuccessiveHalving(10))\n",
    "sdvi.find_slps(100)\n",
    "ground_truth_weights, global_marginal_likelihood = model.calculate_ground_truth_weights(sdvi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_extract_elbos(sweep_dir, method_name, fname=\"elbos.csv\"):\n",
    "    experiment_dirs = [f.path for f in os.scandir(sweep_dir) if f.is_dir()]\n",
    "    dfs = []\n",
    "    for experiment_dir in experiment_dirs:\n",
    "        run_id = os.path.basename(experiment_dir)\n",
    "        if run_id == \".submitit\":\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            data = np.genfromtxt(os.path.join(experiment_dir, fname), delimiter=\",\", names=True)\n",
    "        except OSError:\n",
    "            # File not found\n",
    "            continue\n",
    "\n",
    "        data = data[\"elbos\"]\n",
    "        # For each iteration calcluate error\n",
    "        num_iterations = data.shape[0]\n",
    "\n",
    "        # Create pandas dataframe with iteration,error as columns and append to dfs\n",
    "        errors_pd = pd.DataFrame(data={\"elbos\": data, \"iteration\": np.arange(num_iterations)})\n",
    "        errors_pd[\"run_id\"] = int(run_id)\n",
    "        dfs.append(errors_pd)\n",
    "    \n",
    "    # Concat all dataframes to each other\n",
    "    metrics = pd.concat(dfs)\n",
    "\n",
    "    # Group-by iteration id and calculate mean and standard deviation so that we get a table \"iteration,mean,std\"\n",
    "    grouped_by_iteration = metrics[[\"iteration\", \"elbos\"]].groupby(\"iteration\")\n",
    "    losses_stats = grouped_by_iteration.mean()\n",
    "    losses_stats.rename(columns={\"elbos\": \"mean\"}, inplace=True)\n",
    "    losses_stats[\"std\"] = grouped_by_iteration.std()[\"elbos\"]\n",
    "    losses_stats[\"method_name\"] = method_name\n",
    "    return losses_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdvi_extract_elbos(sweep_dir, method_name, fname=\"exclusive_kl_results.csv\"):\n",
    "    # Loop through the different run idxs\n",
    "    experiment_dirs = [f.path for f in os.scandir(sweep_dir) if f.is_dir()]\n",
    "    dfs = []\n",
    "    for experiment_dir in experiment_dirs:\n",
    "        run_id = os.path.basename(experiment_dir)\n",
    "        if run_id == \".submitit\":\n",
    "            continue\n",
    "\n",
    "        # Load metrics csv into pandas dataframe\n",
    "        metrics = pd.read_csv(os.path.join(experiment_dir, fname))\n",
    "\n",
    "        num_iterations = len(metrics.index)\n",
    "\n",
    "        # Create pandas dataframe with iteration,error as columns and append to dfs\n",
    "        errors_pd = pd.DataFrame(data={\"elbos\": metrics[\"global_elbos\"], \"iteration\": np.arange(num_iterations)})\n",
    "        errors_pd[\"run_id\"] = int(run_id)\n",
    "        dfs.append(errors_pd)\n",
    "\n",
    "    # Concat all dataframes to each other\n",
    "    metrics = pd.concat(dfs)\n",
    "    metrics.describe()\n",
    "\n",
    "    # Group-by iteration id and calculate mean and standard deviation so that we get a table \"iteration,mean,std\"\n",
    "    grouped_by_iteration = metrics[[\"iteration\", \"elbos\"]].groupby(\"iteration\")\n",
    "    losses_stats = grouped_by_iteration.mean()\n",
    "    losses_stats.rename(columns={\"elbos\": \"mean\"}, inplace=True)\n",
    "    losses_stats[\"std\"] = grouped_by_iteration.std()[\"elbos\"]\n",
    "    losses_stats[\"method_name\"] = method_name\n",
    "    return losses_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method2elbos = {\n",
    "    name: baseline_extract_elbos(d, name) \n",
    "    for name, d in baselines \n",
    "    if name in [\"Pyro AutoGuide\", \"BBVI\"]\n",
    "}\n",
    "for n, sweep_dir in sdvi_dirs:\n",
    "    method2elbos[n] = sdvi_extract_elbos(sweep_dir, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_width = 4\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "for name, errors in method2elbos.items():\n",
    "    ixs = errors.index\n",
    "    if name == \"SDVI\":\n",
    "        ixs = ixs * 500 + 2000\n",
    "    elif name == \"Pyro AutoGuide\":\n",
    "        ixs = ixs * 1000\n",
    "    elif name == \"BBVI\":\n",
    "        ixs = ixs * 1000\n",
    "    ax.plot(ixs, errors[\"mean\"], alpha=1.0, lw=line_width, **algo2config[name])\n",
    "    ax.fill_between(ixs, errors[\"mean\"]-errors[\"std\"], errors[\"mean\"]+errors[\"std\"], alpha=0.3, **algo2config[name])\n",
    "\n",
    "ax.axhline(\n",
    "    torch.log(global_marginal_likelihood),\n",
    "    linestyle=\"--\",\n",
    "    color=\"black\",\n",
    "    lw=line_width,\n",
    "    label=r\"$\\log Z$\"\n",
    ")\n",
    "ax.set_xlabel(\"Computational Cost\")\n",
    "ax.set_ylabel(\"ELBO\")\n",
    "# ax.set_ylim((0.0, 0.05))\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlim((1000, 100000))\n",
    "\n",
    "ax.grid(True)\n",
    "ax.legend(loc=\"lower right\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figures/model_selection_elbos.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP Kernel Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Posterior Predictive Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"../data/airline/airline.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_with_median_lppd(sweep_dir, key=\"lppds\", fname=\"exclusive_kl_results.csv\"):\n",
    "    # Loop through the different run idxs\n",
    "    experiment_dirs = [f.path for f in os.scandir(sweep_dir) if f.is_dir()]\n",
    "    dfs = []\n",
    "    for experiment_dir in experiment_dirs:\n",
    "        run_id = os.path.basename(experiment_dir)\n",
    "        if run_id == \".submitit\":\n",
    "            continue\n",
    "        # Load metrics csv into pandas dataframe\n",
    "        metrics = pd.read_csv(os.path.join(experiment_dir, fname))\n",
    "\n",
    "        # Create pandas dataframe with iteration,error as columns and append to dfs\n",
    "        errors_pd = pd.DataFrame(data={key: metrics[key].iloc[-1], \"iteration\": [0]})\n",
    "        errors_pd[\"run_id\"] = int(run_id)\n",
    "        dfs.append(errors_pd)\n",
    "\n",
    "    # Concat all dataframes to each other\n",
    "    metrics = pd.concat(dfs)\n",
    "    print(metrics)\n",
    "    median_iteration_id = metrics.loc[metrics[\"lppds\"] == metrics[\"lppds\"].median()][\"run_id\"][0]\n",
    "    return median_iteration_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_dir = gp_sdvi_result_dirs[1][1]\n",
    "median_lppd_id = get_id_with_median_lppd(sweep_dir)\n",
    "# median_lppd_id = 1\n",
    "with open(os.path.join(sweep_dir, str(median_lppd_id), \"sdvi.pickle\"), \"rb\") as f:\n",
    "    sdvi = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    data = torch.tensor(np.loadtxt(data_path, delimiter=\",\"))\n",
    "    xs = data[:, 0]\n",
    "    ys = data[:, 1]\n",
    "    xs -= xs.min()\n",
    "    xs /= xs.max()\n",
    "    ys -= ys.mean()\n",
    "    ys *= 4 / (ys.max() - ys.min())\n",
    "\n",
    "    # Keep 10 % of data for validation.\n",
    "    val_ix = round(xs.size(0) * 0.9)\n",
    "    xs, xs_val = xs[:val_ix], xs[val_ix:]\n",
    "    ys, ys_val = ys[:val_ix], ys[val_ix:]\n",
    "\n",
    "    return xs, ys, xs_val, ys_val\n",
    "\n",
    "def extract_posterior_kernels(posterior_samples):\n",
    "    post_kernels = [trace.nodes[\"_RETURN\"][\"value\"] for trace in posterior_samples]\n",
    "    for ix in range(len(post_kernels)):\n",
    "        for name, s in posterior_samples[ix].iter_stochastic_nodes():\n",
    "            if name in [\"std\", \"y\"] or \"kernel_type\" in name:\n",
    "                continue\n",
    "\n",
    "            if isinstance(post_kernels[ix], gp.kernels.Sum) or isinstance(\n",
    "                post_kernels[ix], gp.kernels.Product\n",
    "            ):\n",
    "                names = name.split(\".\")\n",
    "                kern_mod = post_kernels[ix]._modules[names[0]]\n",
    "                for jx in range(len(names) - 2):\n",
    "                    kern_mod = kern_mod._modules[names[jx + 1]]\n",
    "                setattr(kern_mod, names[-1], s[\"value\"])\n",
    "            else:\n",
    "                setattr(post_kernels[ix], name, s[\"value\"])\n",
    "    return post_kernels\n",
    "\n",
    "def gp_analytic_posterior(\n",
    "    kernel_fn: gp.kernels.Kernel,\n",
    "    X: torch.tensor,\n",
    "    new_xs: torch.tensor,\n",
    "    y: torch.tensor,\n",
    "    noise: torch.tensor,\n",
    "    jitter: float,\n",
    "    full_cov: bool = False,\n",
    "):\n",
    "    N = X.size(0)\n",
    "    Kff = kernel_fn(X).contiguous()\n",
    "    Kff = Kff.type(X.dtype).clone()\n",
    "    Kff.view(-1)[:: N + 1] += jitter + torch.pow(noise, 2)\n",
    "    Lff = torch.linalg.cholesky(Kff)\n",
    "\n",
    "    gp_post_mean, gp_post_cov = gp.util.conditional(\n",
    "        new_xs, X, kernel_fn, y, Lff=Lff, jitter=jitter, full_cov=full_cov\n",
    "    )\n",
    "    if full_cov:\n",
    "        M = new_xs.size(0)\n",
    "        gp_post_cov = gp_post_cov.contiguous()\n",
    "        gp_post_cov.view(-1, M * M)[:, :: M + 1] += torch.pow(noise, 2)\n",
    "    else:\n",
    "        gp_post_cov = gp_post_cov + torch.pow(noise, 2)\n",
    "    return gp_post_mean, gp_post_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior_samples(\n",
    "    posterior_samples, \n",
    "    X, \n",
    "    y, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    jitter=1e-6, \n",
    "    with_noise=True, \n",
    "    num_eval_points=500,\n",
    "    start_ix_data=0,\n",
    "    figsize=(15, 10)\n",
    "):\n",
    "    post_kernels = extract_posterior_kernels(posterior_samples)\n",
    "    if with_noise:\n",
    "        noises = [trace.nodes[\"std\"][\"value\"] for trace in posterior_samples]\n",
    "    else:\n",
    "        noises = [torch.tensor(0.0) for _ in range(len(posterior_samples))]\n",
    "\n",
    "    new_xs = torch.linspace(0, 1, num_eval_points)\n",
    "    posterior_fs = torch.zeros((len(post_kernels), new_xs.size(0)))\n",
    "    for ix in range(len(post_kernels)):\n",
    "        with torch.no_grad():\n",
    "            gp_post_mean, gp_post_cov = gp_analytic_posterior(\n",
    "                post_kernels[ix],\n",
    "                X,\n",
    "                new_xs,\n",
    "                y,\n",
    "                noises[ix],\n",
    "                jitter,\n",
    "                full_cov=True,\n",
    "            )\n",
    "        posterior_fs[ix, :] = (\n",
    "            dist.MultivariateNormal(gp_post_mean, gp_post_cov).sample().detach()\n",
    "        )\n",
    "\n",
    "    f_post_mean = posterior_fs.mean(dim=0)\n",
    "    f_post_std = posterior_fs.std(dim=0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.plot(new_xs, f_post_mean, color=\"#0072B2\", lw=2)\n",
    "    ax.fill_between(\n",
    "        new_xs,\n",
    "        f_post_mean - 2 * f_post_std,\n",
    "        f_post_mean + 2 * f_post_std,\n",
    "        color=\"#0072B2\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "    num_samples_to_plot = min(0, len(post_kernels))\n",
    "    for ix in range(num_samples_to_plot):\n",
    "        ax.plot(new_xs, posterior_fs[ix, :], color=\"#009E73\", alpha=0.3, linestyle=\"-\")\n",
    "\n",
    "    ax.scatter(X, y, label=\"Observed Data\", color=\"black\")\n",
    "    ax.scatter(X_val, y_val, label=\"Held-Out Data\", marker=\"x\")\n",
    "    ax.set_xlim((X[start_ix_data] - 0.01, 1.01))\n",
    "    ax.set_ylim((y[start_ix_data:].min() - 0.1, ax.get_ylim()[1]))\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    \n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"Number of Passengers\")\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_val, y_val = load_data(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = sdvi.sample_posterior_predictive(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_posterior_samples(\n",
    "    posterior_samples, \n",
    "    X, \n",
    "    y, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    num_eval_points=1000, \n",
    "    start_ix_data=70, \n",
    "    figsize=(8, 5)\n",
    ")\n",
    "fig.savefig(\"figures/gp_posterior_predictive_median_lppd.pdf\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc0cf9eb75f14dfd7d934912c4868fd66671e39dd54480a150159752595feba5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('vi-stochastic-support-s0AmxQUX-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
